2022-07-14T22:58:33,929 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T22:58:33,929 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T22:58:34,013 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T22:58:34,013 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T22:58:34,020 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T22:58:34,020 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T22:58:34,023 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T22:58:34,023 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T22:58:34,034 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T22:58:34,034 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T22:58:34,034 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T22:58:34,034 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T22:58:34,035 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T22:58:34,035 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T22:58:34,035 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T22:58:34,035 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T22:58:34,036 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T22:58:34,036 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T22:58:34,036 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T22:58:34,036 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T22:58:34,042 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T22:58:34,042 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T22:58:34,042 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T22:58:34,042 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T22:58:34,090 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T22:58:34,090 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T22:58:34,090 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T22:58:34,090 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T22:58:34,091 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T22:58:34,091 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T22:58:34,091 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T22:58:34,091 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T22:58:34,092 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T22:58:34,092 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T22:58:34,207 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T22:58:34,207 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T22:58:34,207 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T22:58:34,207 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T22:58:34,209 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T22:58:34,209 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T22:58:34,523 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T22:58:34,524 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]22634
2022-07-14T22:58:34,524 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T22:58:34,525 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T22:58:34,525 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T22:58:34,525 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T22:58:34,531 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T22:58:34,531 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T22:58:34,537 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T22:58:34,540 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657828714540
2022-07-14T22:58:34,540 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657828714540
2022-07-14T22:58:34,562 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T22:58:34,563 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Backend worker process died.
2022-07-14T22:58:34,563 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T22:58:34,564 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-07-14T22:58:34,564 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-07-14T22:58:34,564 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-07-14T22:58:34,565 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-07-14T22:58:34,564 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T22:58:34,565 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T22:58:34,564 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T22:58:34,565 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T22:58:34,565 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T22:58:34,565 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T22:58:34,566 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T22:58:34,566 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-07-14T22:58:34,566 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'coordinator'
2022-07-14T22:58:34,566 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - 
2022-07-14T22:58:34,566 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T22:58:34,566 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-07-14T22:58:34,575 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - 
2022-07-14T22:58:34,576 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T22:58:34,576 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-07-14T22:58:34,576 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     worker.run_server()
2022-07-14T22:58:34,577 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-07-14T22:58:34,577 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-07-14T22:58:34,578 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-07-14T22:58:34,579 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-07-14T22:58:34,579 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-07-14T22:58:34,579 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-07-14T22:58:34,580 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-07-14T22:58:34,581 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-07-14T22:58:34,581 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-07-14T22:58:34,581 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-07-14T22:58:34,582 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T22:58:34,582 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T22:58:34,582 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T22:58:34,583 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T22:58:34,588 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-07-14T22:58:34,591 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-07-14T22:58:34,592 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T22:58:34,592 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T22:58:34,592 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-07-14T22:58:34,592 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.coordinator'
2022-07-14T22:58:34,575 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T22:58:34,575 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T22:58:34,597 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T22:58:34,597 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T22:58:34,597 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T22:58:34,597 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T22:58:34,598 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T22:58:34,598 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T22:58:34,598 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T22:58:34,598 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T22:58:34,599 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T22:58:34,599 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T22:58:34,606 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T22:58:34,606 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T22:58:34,606 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T22:58:34,606 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T22:58:35,600 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T22:58:35,600 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T22:58:35,994 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T22:58:35,995 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]22701
2022-07-14T22:58:35,996 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T22:58:35,996 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-07-14T22:58:35,996 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T22:58:35,996 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-07-14T22:58:35,996 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T22:58:35,996 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T22:58:35,998 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657828715998
2022-07-14T22:58:35,998 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T22:58:35,998 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657828715998
2022-07-14T22:58:36,011 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T22:58:36,012 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Backend worker process died.
2022-07-14T22:58:36,013 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T22:58:36,013 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-07-14T22:58:36,013 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-07-14T22:58:36,013 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T22:58:36,013 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-07-14T22:58:36,013 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T22:58:36,013 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-07-14T22:58:36,014 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T22:58:36,014 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T22:58:36,014 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T22:58:36,014 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T22:58:36,014 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T22:58:36,014 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T22:58:36,014 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T22:58:36,014 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T22:58:36,014 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T22:58:36,015 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T22:58:36,015 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T22:58:36,015 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T22:58:36,015 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T22:58:36,015 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T22:58:36,015 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T22:58:36,015 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T22:58:36,015 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-07-14T22:58:36,016 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T22:58:36,016 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T22:58:36,016 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T22:58:36,016 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T22:58:36,021 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T22:58:36,021 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T22:58:36,232 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T22:58:36,232 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:05:22,085 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:05:22,085 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:05:22,165 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:05:22,165 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:05:22,172 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:05:22,172 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:05:22,176 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:05:22,176 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:05:22,187 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:05:22,187 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:05:22,188 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:05:22,188 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:05:22,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:05:22,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:05:22,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:05:22,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:05:22,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:05:22,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:05:22,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:05:22,190 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:05:22,196 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:05:22,196 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:05:22,195 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:05:22,195 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:05:22,238 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:05:22,238 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:05:22,238 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:05:22,238 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:05:22,239 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:05:22,239 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:05:22,240 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:05:22,240 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:05:22,240 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:05:22,240 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:05:22,356 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:05:22,356 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:05:22,356 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:05:22,356 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:05:22,357 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:05:22,357 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:05:22,665 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:05:22,666 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]23721
2022-07-14T23:05:22,666 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:05:22,666 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:05:22,667 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:05:22,667 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:05:22,672 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:05:22,672 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:05:22,679 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:05:22,681 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829122681
2022-07-14T23:05:22,681 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829122681
2022-07-14T23:05:22,705 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:05:22,732 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Backend worker process died.
2022-07-14T23:05:22,732 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T23:05:22,733 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-07-14T23:05:22,733 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-07-14T23:05:22,733 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:05:22,734 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-07-14T23:05:22,733 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:05:22,734 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-07-14T23:05:22,734 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T23:05:22,735 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:05:22,735 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:05:22,735 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T23:05:22,735 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T23:05:22,735 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T23:05:22,736 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-07-14T23:05:22,736 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-07-14T23:05:22,736 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-07-14T23:05:22,745 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-07-14T23:05:22,745 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/tmp/models/bfca37c3a4cc4bc68b457887a5c83935/coordinator.py", line 7, in <module>
2022-07-14T23:05:22,746 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     from cv2 import cv
2022-07-14T23:05:22,746 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - ImportError: cannot import name 'cv' from 'cv2' (/home/jafar/anaconda3/lib/python3.9/site-packages/cv2/__init__.py)
2022-07-14T23:05:22,746 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - 
2022-07-14T23:05:22,746 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-07-14T23:05:22,747 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - 
2022-07-14T23:05:22,747 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T23:05:22,747 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-07-14T23:05:22,748 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     worker.run_server()
2022-07-14T23:05:22,748 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-07-14T23:05:22,748 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-07-14T23:05:22,748 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-07-14T23:05:22,749 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-07-14T23:05:22,749 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-07-14T23:05:22,749 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-07-14T23:05:22,750 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 102, in load
2022-07-14T23:05:22,750 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-07-14T23:05:22,750 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-07-14T23:05:22,751 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-07-14T23:05:22,751 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T23:05:22,751 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T23:05:22,751 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T23:05:22,752 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T23:05:22,752 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
2022-07-14T23:05:22,752 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-07-14T23:05:22,752 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T23:05:22,752 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T23:05:22,753 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
2022-07-14T23:05:22,753 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.coordinator'
2022-07-14T23:05:22,735 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:05:22,735 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:05:22,758 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:05:22,758 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:05:22,758 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:05:22,759 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:05:22,759 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:05:22,765 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:05:22,765 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:05:22,765 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:05:22,765 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:05:23,760 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:05:23,760 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:05:24,184 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:05:24,185 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]23782
2022-07-14T23:05:24,186 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-07-14T23:05:24,186 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:05:24,186 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-07-14T23:05:24,186 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:05:24,186 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:05:24,186 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:05:24,188 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829124188
2022-07-14T23:05:24,188 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:05:24,188 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829124188
2022-07-14T23:05:24,197 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:05:24,225 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Backend worker process died.
2022-07-14T23:05:24,225 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T23:05:24,225 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-07-14T23:05:24,225 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:05:24,225 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-07-14T23:05:24,225 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:05:24,226 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-07-14T23:05:24,226 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-07-14T23:05:24,226 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:05:24,226 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:05:24,226 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T23:05:24,226 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T23:05:24,226 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:05:24,227 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T23:05:24,226 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:05:24,227 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T23:05:24,227 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:05:24,227 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:05:24,227 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-07-14T23:05:24,227 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:05:24,227 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:05:24,227 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-07-14T23:05:24,228 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:05:24,228 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-07-14T23:05:24,228 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:05:24,228 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:05:24,228 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-07-14T23:05:24,228 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:05:24,228 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/tmp/models/bfca37c3a4cc4bc68b457887a5c83935/coordinator.py", line 7, in <module>
2022-07-14T23:05:24,229 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:05:24,229 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:05:24,229 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:05:24,229 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:05:24,237 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:05:24,237 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:05:24,374 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:05:24,374 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:06:22,458 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:06:22,458 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:06:22,540 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:06:22,540 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:06:22,548 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:06:22,548 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:06:22,552 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:06:22,552 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:06:22,564 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:06:22,564 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:06:22,564 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:06:22,564 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:06:22,566 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:06:22,566 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:06:22,566 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:06:22,573 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:06:22,573 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:06:22,573 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:06:22,573 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:06:22,617 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:06:22,617 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:06:22,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:06:22,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:06:22,618 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:06:22,618 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:06:22,618 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:06:22,618 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:06:22,619 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:06:22,619 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:06:22,732 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:06:22,732 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:06:22,732 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:06:22,732 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:06:22,733 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:06:22,733 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:06:23,032 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:06:23,034 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]23983
2022-07-14T23:06:23,034 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:06:23,034 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:06:23,034 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:06:23,034 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:06:23,037 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:06:23,037 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:06:23,043 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:06:23,046 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829183046
2022-07-14T23:06:23,046 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829183046
2022-07-14T23:06:23,066 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:06:23,335 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - STUFF IN THE DIRECTORY
2022-07-14T23:06:23,335 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Backend worker process died.
2022-07-14T23:06:23,336 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-07-14T23:06:23,336 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:06:23,336 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-07-14T23:06:23,336 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-07-14T23:06:23,337 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     worker.run_server()
2022-07-14T23:06:23,337 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:06:23,337 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-07-14T23:06:23,337 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-07-14T23:06:23,337 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-07-14T23:06:23,347 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-07-14T23:06:23,348 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-07-14T23:06:23,348 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-07-14T23:06:23,348 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-07-14T23:06:23,348 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 100, in load
2022-07-14T23:06:23,349 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-07-14T23:06:23,349 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-07-14T23:06:23,350 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-07-14T23:06:23,353 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/home/jafar/anaconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
2022-07-14T23:06:23,353 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-07-14T23:06:23,353 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 850, in exec_module
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -   File "/tmp/models/8fce3134b7634c10b6421322fb615971/coordinator.py", line 19, in <module>
2022-07-14T23:06:23,354 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG -     print(os.listdir('models'))
2022-07-14T23:06:23,355 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - FileNotFoundError: [Errno 2] No such file or directory: 'models'
2022-07-14T23:06:23,337 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:06:23,337 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: all_det, error: Worker died.
2022-07-14T23:06:23,360 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:06:23,360 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stderr
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:06:23,360 [WARN ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-all_det_1.0-stdout
2022-07-14T23:06:23,361 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:06:23,361 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-07-14T23:06:23,376 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:06:23,376 [INFO ] W-9000-all_det_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stdout
2022-07-14T23:06:23,376 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:06:23,376 [INFO ] W-9000-all_det_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-all_det_1.0-stderr
2022-07-14T23:06:24,362 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:06:24,362 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:06:24,755 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:06:24,755 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:07:00,473 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:00,473 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:00,569 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:00,569 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:00,576 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:00,576 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:00,580 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:07:00,580 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:07:00,593 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:00,593 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:00,594 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:00,594 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:00,595 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:07:00,595 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:07:00,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:07:00,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:07:00,596 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:07:00,596 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:07:00,596 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:07:00,596 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:07:00,602 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:07:00,602 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:07:00,602 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:07:00,602 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:07:00,656 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:07:00,656 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:07:00,656 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:07:00,656 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:07:00,657 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:07:00,657 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:07:00,657 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:07:00,657 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:07:00,658 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:07:00,658 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:07:00,771 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:07:00,771 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:07:00,771 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:07:00,771 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:07:00,772 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:07:00,772 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:07:01,069 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:07:01,069 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]24200
2022-07-14T23:07:01,070 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:07:01,070 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:07:01,070 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:07:01,070 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:07:01,075 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:07:01,075 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:07:01,083 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:07:01,085 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829221085
2022-07-14T23:07:01,085 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829221085
2022-07-14T23:07:01,108 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:07:02,794 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:07:02,794 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:07:46,418 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:46,418 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:46,484 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:46,484 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:46,492 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:46,492 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:46,495 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:07:46,495 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:07:46,507 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:46,507 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:46,507 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:46,507 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:07:46,509 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:07:46,509 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:07:46,509 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:07:46,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:07:46,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:07:46,515 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:07:46,515 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:07:46,557 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:07:46,557 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:07:46,557 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:07:46,557 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:07:46,558 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:07:46,558 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:07:46,559 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:07:46,559 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:07:46,559 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:07:46,559 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:07:46,674 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:07:46,674 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:07:46,674 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:07:46,674 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:07:46,675 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:07:46,675 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:07:46,956 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:07:46,957 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]24378
2022-07-14T23:07:46,957 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:07:46,957 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:07:46,958 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:07:46,958 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:07:46,963 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:07:46,963 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:07:46,970 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:07:46,973 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829266973
2022-07-14T23:07:46,973 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829266973
2022-07-14T23:07:46,994 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:07:48,696 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:07:48,696 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:07:59,909 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:59,909 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:07:59,980 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:59,980 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:07:59,988 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:59,988 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:07:59,990 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:07:59,990 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:08:00,005 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:00,005 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:00,005 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:00,005 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:00,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:08:00,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:08:00,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:08:00,006 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:08:00,007 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:08:00,007 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:08:00,007 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:08:00,007 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:08:00,013 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:08:00,013 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:08:00,013 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:08:00,013 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:08:00,051 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:08:00,051 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:08:00,051 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:08:00,051 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:08:00,052 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:08:00,052 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:08:00,053 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:08:00,053 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:08:00,053 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:08:00,053 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:08:00,167 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:08:00,167 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:08:00,167 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:08:00,167 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:08:00,168 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:08:00,168 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:08:00,483 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:08:00,484 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]24528
2022-07-14T23:08:00,484 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:08:00,485 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:08:00,485 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:08:00,485 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:08:00,490 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:08:00,490 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:08:00,498 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:08:00,500 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829280500
2022-07-14T23:08:00,500 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829280500
2022-07-14T23:08:00,523 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:08:02,184 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:08:02,184 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:08:41,710 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:08:41,710 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:08:41,780 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:08:41,780 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:08:41,788 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:08:41,788 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:08:41,791 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:08:41,791 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:08:41,805 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:41,805 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:41,805 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:41,805 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:08:41,807 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:08:41,807 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:08:41,807 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:08:41,814 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:08:41,814 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:08:41,813 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:08:41,813 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:08:41,867 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:08:41,867 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:08:41,867 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:08:41,867 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:08:41,868 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:08:41,868 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:08:41,868 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:08:41,868 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:08:41,869 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:08:41,869 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:08:41,984 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:08:41,984 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:08:41,984 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:08:41,984 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:08:41,986 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:08:41,986 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:08:42,248 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:08:42,250 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]24760
2022-07-14T23:08:42,250 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:08:42,250 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:08:42,251 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:08:42,251 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:08:42,256 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:08:42,256 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:08:42,263 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:08:42,265 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829322265
2022-07-14T23:08:42,265 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829322265
2022-07-14T23:08:42,283 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:08:44,009 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:08:44,009 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:09:24,568 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:09:24,568 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:09:24,640 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:09:24,640 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:09:24,648 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:09:24,648 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:09:24,650 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:09:24,650 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:09:24,667 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:09:24,667 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:09:24,667 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:09:24,667 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:09:24,668 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:09:24,668 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:09:24,668 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:09:24,668 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:09:24,668 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:09:24,668 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:09:24,669 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:09:24,669 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:09:24,675 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:09:24,675 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:09:24,675 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:09:24,675 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:09:24,721 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:09:24,721 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:09:24,722 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:09:24,722 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:09:24,723 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:09:24,723 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:09:24,723 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:09:24,723 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:09:24,724 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:09:24,724 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:09:24,849 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:09:24,849 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:09:24,849 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:09:24,849 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:09:24,851 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:09:24,851 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:09:25,128 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:09:25,129 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]25111
2022-07-14T23:09:25,130 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:09:25,130 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:09:25,130 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:09:25,130 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:09:25,135 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:09:25,135 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:09:25,143 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:09:25,145 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829365145
2022-07-14T23:09:25,145 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657829365145
2022-07-14T23:09:25,167 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:09:26,875 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:09:26,875 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:37:44,692 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:37:44,692 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:37:44,761 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:37:44,761 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:37:44,767 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:37:44,767 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:37:44,770 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:37:44,770 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:37:44,784 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:37:44,784 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:37:44,784 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:37:44,784 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:37:44,785 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:37:44,785 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:37:44,785 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:37:44,791 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:37:44,791 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:37:44,792 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:37:44,792 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:37:44,834 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:37:44,834 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:37:44,834 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:37:44,834 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:37:44,835 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:37:44,835 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:37:44,836 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:37:44,836 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:37:44,837 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:37:44,837 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:37:44,954 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:37:44,954 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:37:44,954 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:37:44,954 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:37:44,955 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:37:44,955 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:37:45,279 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:37:45,281 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]46477
2022-07-14T23:37:45,281 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:37:45,281 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:37:45,282 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:37:45,282 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:37:45,287 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:37:45,287 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:37:45,294 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:37:45,296 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831065296
2022-07-14T23:37:45,296 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831065296
2022-07-14T23:37:45,315 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:37:46,978 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:37:46,978 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:39:30,462 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:39:30,462 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:39:30,545 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:39:30,545 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:39:30,552 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:39:30,552 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:39:30,554 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:39:30,554 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:39:30,566 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:39:30,566 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:39:30,566 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:39:30,566 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:39:30,568 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:39:30,568 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:39:30,568 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:39:30,574 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:39:30,574 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:39:30,574 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:39:30,574 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:39:30,616 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:39:30,616 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-14T23:39:30,616 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:39:30,616 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:39:30,617 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:39:30,617 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-07-14T23:39:30,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:39:30,617 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:39:30,618 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:39:30,618 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-07-14T23:39:30,739 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:39:30,739 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:39:30,739 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:39:30,739 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:39:30,740 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:39:30,740 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:39:31,004 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:39:31,005 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]47077
2022-07-14T23:39:31,006 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:39:31,006 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:39:31,006 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:39:31,006 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:39:31,010 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:39:31,010 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:39:31,018 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:39:31,020 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831171020
2022-07-14T23:39:31,020 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831171020
2022-07-14T23:39:31,040 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:39:32,766 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:39:32,766 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:46:04,242 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:46:04,242 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-14T23:46:04,320 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://127.0.0.1:9001
Management address: http://127.0.0.1:9002
Metrics address: http://127.0.0.1:9003
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:46:04,320 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/jafar/anaconda3/lib/python3.9/site-packages
Current directory: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 3982 M
Python executable: /home/jafar/anaconda3/bin/python
Config file: configs.properties
Inference address: http://127.0.0.1:9001
Management address: http://127.0.0.1:9002
Metrics address: http://127.0.0.1:9003
Model Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Initial Models: all
Log dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Metrics dir: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 16777216
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/jafar/PyCharmProjects/Upwork/Andrei-Torchserve/torchserve/serve_alldet
Model config: {"all_det": {"1.0": {"defaultVersion": true,"marName": "all_det.mar","minWorkers": 1,"maxWorkers": 1,"batchSize": 1,"job_queue_size" : 33,"maxBatchDelay": 300,"responseTimeout": 300,"maxRequestSize": 16777216}}}
2022-07-14T23:46:04,327 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:46:04,327 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-14T23:46:04,330 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:46:04,330 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: all_det.mar
2022-07-14T23:46:04,345 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:46:04,345 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:46:04,346 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:46:04,346 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model all_det
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model all_det
2022-07-14T23:46:04,348 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:46:04,348 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model all_det loaded.
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:46:04,348 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: all_det, count: 1
2022-07-14T23:46:04,356 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:46:04,356 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-14T23:46:04,356 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:46:04,356 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/jafar/anaconda3/bin/python, /home/jafar/anaconda3/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-07-14T23:46:04,399 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:9001
2022-07-14T23:46:04,399 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:9001
2022-07-14T23:46:04,400 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:46:04,400 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:9002
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:9002
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:9003
2022-07-14T23:46:04,401 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:9003
2022-07-14T23:46:04,524 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:46:04,524 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:46:04,524 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-07-14T23:46:04,524 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-07-14T23:46:04,525 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:46:04,525 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-07-14T23:46:04,778 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-07-14T23:46:04,780 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - [PID]50358
2022-07-14T23:46:04,780 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-14T23:46:04,780 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Python runtime: 3.9.7
2022-07-14T23:46:04,781 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:46:04,781 [DEBUG] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-all_det_1.0 State change null -> WORKER_STARTED
2022-07-14T23:46:04,785 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:46:04,785 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-07-14T23:46:04,793 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-07-14T23:46:04,796 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831564796
2022-07-14T23:46:04,796 [INFO ] W-9000-all_det_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1657831564796
2022-07-14T23:46:04,817 [INFO ] W-9000-all_det_1.0-stdout MODEL_LOG - model_name: all_det, batchSize: 1
2022-07-14T23:46:06,546 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2022-07-14T23:46:06,546 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
